\documentclass[11pt]{article}
\input{template}

\usepackage{url}
\usepackage{natbib}
\usepackage{hyperref}
\newcommand{\mailto}[1]{\href{mailto:#1}{#1}}

\setcitestyle{comma,numbers,sort&compress,square}

\title{Semantic Delta-Debugging}
\author{Andrew Coonce, Suhail Shergill, Tristan Ravitch \\
\{\mailto{coonce}, \mailto{shergill}, \mailto{travitch}\}@cs.wisc.edu
}
\begin{document}
% \special{papersize=8.5in,11in}
% \setlength{\pdfpageheight}{\paperheight}
% \setlength{\pdfpagewidth}{\paperwidth}
% You may need to change the horizontal offset to do what you
% want.  Setting \hoffset to a negative value moves all printed
% material to the left on all pages; setting it to a positive value
% moves all printed material to the right on all pages; not setting
% it keeps all printed material in it's default position.  \voffset
% is the vertical offset: use negative value for up; don't set if
% you want to use default position; use positive for down.
% \hoffset = -0.2truein
% \voffset = -0.2truein
\maketitle

\section{Overview}
\label{sec:intro}
The Delta Debugging algorithm introduced by \citet{dd} works well
for many kinds of inputs. The algorithm is, additionally, simple to understand
and implement. It performs an efficient search over the exponential state space
of input programs to find minimal failing inputs. The algorithm is intentionally
uninformed about the underlying structure of inputs to keep it decoupled from
the semantics of any particular programming language.

Unfortunately, this decoupling can be self-defeating for input files with rich
internal dependencies. As an example, consider the case where the Delta
Debugging algorithm removes a base class from which several others derive in a
C++ source file. Removing, or just breaking, any class in a long inheritance
chain will produce a program that does not compile at all. The Delta Debugging
algorithm is actually designed to tolerate many failures of this form. On the
other hand, there must be at least some successes in order to reduce the input
size by any appreciable factor. In the presence of numerous internal
dependencies, it is likely that no viable reductions will ever emerge. Cases
have been observed \citep{trex} where the Delta Debugging algorithm ran for more than three
days and was unable to reduce 700kb input files by more than $20 \%$. In these
situations, the Delta Debugging algorithm only managed to remove whitespace.
Manual reduction, respecting class hierarchies, was able to reduce these
test cases to 40kb.  The standard ddmin algorithm was able to further reduce
these simplified cases.

This project aims to introduce the simple concept of \emph{dependency} to the
Delta Debugging algorithm to prune impossible inputs from the search space.
This will both reduce the search time and make possible some reductions that
would never emerge under traditional Delta Debugging. The algorithm will only be
altered to take into account the notion of dependency constraints between
character ranges of the input. A different front-end will be required for each
structurally complex input format (C++, Python, Java), but the algorithm itself
will remain ignorant of the semantics any particular input language.

An advantage of basing this work on the Delta Debugging algorithm is that it
does not have to be precise or $100 \%$ accurate. Some amount of robustness
against inaccuracy in the front-end is inherent to the algorithm and the search
will continue around improperly captured dependencies.  The resulting algorithm
will not be in a worse position than the original Delta Debugging algorithm with
regard to spurious syntax errors.  Our goal is only to reduce the number of
failures over the baseline of Delta Debugging. This structured approach will
allow us to effectively explore the space of mostly well-formed input programs
and introduce reductions unavailable to traditional Delta Debugging.


\section{Implementation Details}
We will decouple the actual algorithm, which deals with tracking dependencies
and choosing sections of code to eliminate, from the process of generating said
dependencies.

\begin{itemize}
\item The \emph{front-end} will be responsible for parsing source files and
  generating semantic dependencies between source ranges. We will be working
  with Clang \citep{clang} (which is a C/C++ front-end) and would use Clang libraries to
  output structural constraints over C/C++ code.  This approach would generate
  syntactic and semantic dependency constraints over \emph{source ranges}.  After
  some minor processing, Clang provides precise character-level source ranges for
  all constructs in which we are interested

\item The constraint in which we have a primary interest is \emph{dependsOn(X,
  Y)} (i.e., construct X depends on construct Y).  We have identified various
  dependencies:

\begin{itemize}
\item{Variable and function \emph{references} depend on their declarations.}
\item{Variable and function \emph{declarations} depend on the declaration of the
  type of the variable.}
\item{\emph{Typedef} aliases depend on the declaration of the aliased type.}
\item{Dependencies introduced by the \emph{transitive} closure of the dependency
  relation.
}
\end{itemize}

\item To take advantage of the dependencies, we will investigate new search
  strategies beyond that employed by traditional Delta Debugging.  The goal of
  these strategies is to remove constructs (and their dependencies) in the order
  which most quickly isolates the failure-inducing code until a minimally
  failing input has been found.  Every time a construct is removed, anything
  which depends on it is also (transitively) removed.  Beyond removal, we may
  also investigate more general transformations of constructs.

  We will use a Datalog or Prolog implementation to compute the transitive
  closure of removed constructs based on our generated dependency constraints.
  With some postprocessing, we can reconstruct the remaining program source
  ranges and follow the standard Delta Debugging algorithm in running the
  debuggee on the new input and observing either failures or successes.

\end{itemize}

\subsection{Search Strategies}
The search strategy can be approached using either common approach:
\begin{itemize}
\item \emph{Top-down}: Start by removing top-level declarations (and things that
  require them). Start removing smaller entities (statements) once no more
  progress can be made there, iterating as necessary.
\item \emph{Bottom-up}: Start removing non-declaration statements aggressively
  as in the search strategy from Zeller \citet{dd}. After no progress can be
  made, start removing larger constructs by working at the next higher stratum.
\end{itemize}
It is not yet clear which method is superior, but the proposed dependency
modifications should make the bottom-up search strategy a more attractive option
than it is in a more traditional, dependency-unaware Delta Debugging.

Delta Debugging typically starts off with a standard binary search. This is
possible under a structured framework, too. To that end we may consider some
heuristics and approximate techniques from graph cutting and image segmentation
literature \citep{nc} in order to keep us from making sweeping changes (eg.,
removing a base class on which all classes may depend).

\subsection{Transformations}
We may be able to do better than simply removing program constructs if we have
dependency (or possibly even richer) information.  Some possibilities are:
\begin{itemize}
\item Type simplification.  We could replace complex class types with simple
  primitives.
\item Function body removal, essentially replacing a function definition with a
  declaration.
\item Remove a struct, class, or union field.
\item Remove template specializations.
\end{itemize}

Removing entire entities is simple in this framework, since we will record the
source ranges of all constructs.  Removing function bodies could be more
difficult because just removing the curly braces of the body and everything
in-between is insufficient; the body needs to be replaced by a semicolon to
produce a syntactically valid declaration in C.  Something like this is possible
with some extra semantic annotations within the source ranges. We could specify
a list of substitutions for some ranges that would just be empty for most
constructs; for a function body the list could contain a semicolon.  A related
example could be replacing a complex conditional in a loop with a simple
constant.

Type simplification would be difficult to achieve automatically and will
probably be beyond the scope of the current project.

\section{Evaluation}
Our evaluation will proceed in two stages.  First, we will compare the
number of iterations required to find a minimal input by our algorithm
and the ddmin algorithm of \citeauthor{dd}.  The final output of our
algorithm may be slightly larger as we will not be shortening variable
names or explicitly discarding whitespace.  We will ensure that our
outputs are similarly semantically minimal.  We will perform these tests
on C-based test cases where ddmin is known to reduce the input size
appreciably.

Next, we will examine larger test cases with complex internal
dependencies for which ddmin is unsuitable.  Ideally, we could do this
using the original 700kb input file discussed in section
\ref{sec:intro}.  This may not be possible if we are not able to cover
enough of the C++ language to generate all of the constraints for this
rather complex file.  If this is the case, we will generate larger C
files with complex dependencies between structures using composition
instead of inheritance.  This will capture the spirit of the
underlying problem without requiring the full machinery to handle all
of C++.  Our goal with these inputs will be to show that our algorithm
can reduce the input size appreciably in a reasonable number of
iterations in cases where ddmin fails to converge after several days.

\section{Team Members}
\begin{itemize}
\item{Andrew Coonce}
\item{Suhail Shergill}
\item{Tristan Ravitch}
\end{itemize}

\section{Deliverables Proposed Schedule}
Our deliverables schedule is dictated in part by the program flow and has the
benefit of front-loading most of the code-intensive segments. The project is
viewed as consisting of three main components: Constraint Generation (budget: 4
of our 7 weeks), Querying Framework (budget: 2 weeks), and the Testing Platform
that unifies them (budget: 1 week). While optimistic, this schedule should
provide enough leniency that overruns should be unlikely.  \\ \\
\begin{tabular}{|l|l|l|}
\hline
\multicolumn{3}{|c|}{Timeline} \\
\hline
\multirow{2}{*}{Constraint Generation} & 11/12 &
Parse the source file and determine semantic \\
 & & dependency mappings \\ \hline
\multirow{4}{*}{Querying Framework} & 11/19 & Compose a graph partitioning
strategy that \\
 & & approximates a binary search \\
 & 11/26 & Compute transitive closure of dependency relationships \\
 & & over the search space \\ \hline
\multirow{2}{*}{Testing Platform} & 12/03 & Perform the Symbolic $\rightarrow$
Source Line mappings and \\
 & & generate cases for failure isolation testing \\ \hline
\end{tabular}



\section{Related Work}
There has been some prior work in the area with, perhaps, the one most closely
related to ours being the Hierarchical Delta Debugging algorithm as proposed by
\citet{hdd} which works on tree-structured input. However, while
the algorithm can operate on the AST of a program it is unable to account for
dependencies arising from non-ancestral nodes due to declarations.

There have been other extensions to the delta debugging algorithm as well, with
variants such as Artho's Iterative Delta Debugging \citep{idd} directing a search
along the axis of version control commits. \citet{smt} used a
variant of the Hierarchical Delta-Debugging algorithm, using the knowledge of
formula structures and types to speed up the delta-debugging process on SMT
formulas.

Our constraint generation step can also be seen as a special case of program
slicing \citep{weiser81} \citep{tip94}. These techniques ease debugging by
removing irrelevant portions of a failing program and can be done either
statically or dynamically (with respect to a concrete run) \citep{agrawal90}. One
strength of our technique is that in combining program slicing with a technique
as failure resistant as delta-debugging we do not need to be as precise in our
analysis.


\section{Future Work}
Our approach can also potentially be extended to other languages eg., Python,
OCaml etc. Python, being dynamically typed, would be challenging since we would
have to perform an interprocedural dataflow analysis to generate the
dependencies. It would still be feasible, however, since we do not need to be
either sound nor complete. At the same time using Python's AST module as a
front-end would make certain things easier too, as we could work at the AST
level and convert it to executable code directly without having to deal with
source ranges, or even program text.

\bibliographystyle{abbrvnat}
\bibliography{refs}

\end{document}
