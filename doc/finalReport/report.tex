
\documentclass[preprint]{acm_proc_article-sp}

\usepackage{url}
\usepackage{natbib}
\usepackage{hyperref}

\usepackage{algorithmic}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\setcitestyle{comma,numbers,sort&compress,square}

\begin{document}
\title{Semantic Delta-Debugging}

\numberofauthors{2}

\author{
\alignauthor
Suhail Shergill\\
\email{shergill@cs.wisc.edu}
\alignauthor
Andrew Coonce\\
\email{coonce@cs.wisc.edu}
}

\date{}

\maketitle
\begin{abstract}
Delta Debugging is a general debugging technique which is both simple to
understand and can be used to quickly find minimal versions of failure-inducing
input. Since the algorithm is generic it is intentionally uninformed about the
underlying structure of its input and is decoupled from the semantics of any
particular programming language. Unfortunately, this decoupling means that a
standard Delta Debugging implementation struggles on input files with rich
internal dependency structures.

In this project, we extend Delta Debugging by introducing dependency awareness
as a method of constraining the search space. We have modified the algorithm to
take into account dependencies between various source ranges within the input
file and prune impossible inputs that contain broken dependencies from the
search space. As a proof-of-concept, we provide a front-end dependency generator
for a subset of C and demonstrate the improvements of the now dependency-aware
Semantic Delta Debugging as compared to naive Delta Debugging.
\end{abstract}

%% % A category with the (minimum) three required fields
%% \category{H.4}{Information Systems Applications}{Miscellaneous}
%% %A category including the fourth, optional field follows...
%% \category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%% \terms{Theory}

\keywords{Debugging, Delta-debugging} % NOT required for Proceedings

\section{Introduction}
\label{sec:intro}
FIXME:coonce

The Delta Debugging algorithm introduced by \citet{dd} works well for many kinds
of inputs. The algorithm is, additionally, simple to understand and to
implement. It performs an efficient search over the exponential state space of
input programs to find minimal failing inputs. The algorithm is intentionally
uninformed about the underlying structure of its input files in order to keep it
decoupled from the semantics of any particular programming language. 

Unfortunately, this decoupling can be self-defeating for input files with rich
internal dependencies. As an example, consider the case where the Delta
Debugging algorithm removes the declaration of a base class, upon which several
other derivative classes rely. By removing the class which the other classes
depend upon, the algorithm has broken the code in multiple places. Even if none
of the affected classes are relevant to the reduction, the code does not compile
and, as such, no inference can be made about its relevance to the test-case.

Reemoving, or just breaking, any code segment upon which others depend will
often result in a program that does not compile at all. While the Delta
Debugging algorithm is actually designed to tolerate many failures of this form,
there must be at least some successes in order to reduce the input size by any
appreciable factor. In the presence of numerous internal dependencies, it is
likely that no viable reductions will ever emerge. Cases have been observed
\citep{trex} where the Delta Debugging algorithm ran for more than three days
and was unable to reduce 700kb input files by more than $20 \%$ where manual
reduction was able to reduce the size by more than $90 \%$. Frequently, in
these situations, the Delta Debugging algorithm only manages to remove the
comments and whitespace from the file. Reductions of this sort may actually lead
to the reduced test-cases being more difficult to understand, the exact opposite
of the algorithms intent.

This project aims to introduce the simple concept of \emph{dependency} to the
Delta Debugging algorithm to prune impossible inputs from the search space.
This will both reduce the search time and make possible some reductions that
would never emerge under traditional Delta Debugging. The algorithm will only be
altered to take into account the notion of dependency constraints between
character ranges of the input. A different front-end will be required for each
structurally complex input format (C++, Python, Java), but the algorithm itself
will remain ignorant of the semantics any particular input language.

One compelling benefit of basing this work on the Delta Debugging algorithm is
that it is able to take advantage of the inherent robustness of the algorithm
in dealing with poorly managed dependencies. In the case where the solution
misses a dependency, the worst-case behavior of the algorithm is equivalent to
the original Delta Debugging algorithm's handling of spurious syntax errors.
To that end, our solution's is able to reduce the number of \emph{inconsistent
configurations} \citep{zeller99} arising due to semantic errors over the
baseline Delta Debugging but makes no serious attempt to entirely remove them.
The structured approach followed by our solution effectively explores the space
of mostly well-formed input programs and allows new reductions previously
unavailable to traditional Delta Debugging.

Our preliminary results show... FIXME: SSS

%% SSS
%%   What do our results show? Can we distill the essence down to a short
%%   paragraph and append it to the end of the introduction
%% ALC

\section{Implementation Details}

In order to ensure that the solution we have provided is more easily applicable
to other languages, strong efforts were made to isolate as much of the language
specific code as possible. In light of that goal, we have divided our
implementation into three main components:

\begin{itemize}
\item A \emph{Constraint Generator}, which is responsible for finding and
      annotating constraints within the input file.
\item An \emph{Inference System}, which is is responsible for
      computing the transitive closure of the dependency map.
\item The \emph{Semantic Delta-Debugger}, which constructs and evaluates the
      test-cases.
\end{itemize}

\subsection{Constraint Generator}

The \emph{Constraint Generator} is the front-end system responsible for parsing
source files and detecting semantic dependencies between source ranges. While
this stage uses C/C++ language-specific code to parse the input files, the
output files it generates for later inference analysis use a language agnostic
file format. Whereas traditional Delta Debugging frequently parses input files
at the line- or character-level, our implementation works at the statement/sub-
statement level as seen in Hierarchical Delta Debugging \citep{hdd} in order to
be able to gather the language-specific dependency information.

This aspect of the solution is built with the C/C++ front-end provided by
\emph{Clang} \citep{clang} and uses the Clang libraries to output structural
constraints over the source code file. While the implementation does not support
reasoning about all dependency generation constraints it does fully support many
common constructs including: variables, functions, struct members, typedefs,
enums, and generic statement and sub-statement level expressions.

Each of the aforementioned constructs is given a unique identifier and a source-
range by the Constraint Generator, allowing it to be independently removed
during later processing stages. For most of the tagged objects, simply deleting
the specified source range (or, equivalently, substituting a semi-colon for the
source range) is sufficient to emulate removing the delta during the Semantic
Delta Debugging stage. For certain sub-statement level expressions, there is
sometimes additional processing involved. For example, it is possible to remove
just the conditional expression from the declaration of an 'if loop'. Our
system flags the removed sub-statement expression as a conditional expression
and instead replaces it arbitrarily with a value of 'true' or 'false'. Other
statements types that receive similar treatment include: 'do' loops, 'while'
loops, 'for' loops, 'if-then' and 'if-then-else' blocks.

Beyond creating the unique identifiers and determining the appropriate source
range and replacement information for each of the encountered objects, the
Constraint Generator is also responsible for tracking any and all references to
defined elements. Some common, example relationships that the system can detect
include:

\begin{itemize}

\item The dependency of a variable declaration upon the prior declaration of the
      type of the new variable.
\item The dependency of a variable reference upon it's prior declaration.
\item The dependency of a struct object member field reference upon both the
      prior declaration of the struct object as well as, more specifically, the
      declaration of the specified member field within that struct.

\end{itemize}

While the system is capable of recognizing a large variety of different
situations that envoke dependent relationships between the enabling and
dependent elements of its symbol table, it is not guaranteed to catch all
potential dependency relationships. Though common dependency relations are
tracked, there are certain boundary cases (such as the nested declaration of a
function within the scope of another function) which are uncaught by the system.

Again, the omission of certain dependency relationships only impacts the degree
of reduction for a given test-case and not whether any reduction is possible.
General purpose users will likely find the provided solution sufficient, though
extending the Constraint Generation phase to handle new AST node types is as
simple as extending a few Visitor-pattern classes.

\subsection{Inference System}

\subsection{Semantic Delta-Debugger}

\begin{itemize}

\item To take advantage of the dependencies, we will investigate new search
  strategies beyond that employed by traditional Delta Debugging.  The goal of
  these strategies is to remove constructs (and their dependencies) in the order
  which most quickly isolates the failure-inducing code until a minimally
  failing input has been found.  Every time a construct is removed, anything
  which depends on it is also (transitively) removed.

  We will use a Datalog or Prolog implementation to compute the transitive
  closure of removed constructs based on our generated dependency constraints.
  With some postprocessing, we can reconstruct the remaining program source
  ranges and follow the standard Delta Debugging algorithm by running the
  checker on the newly constructed program and observing the results.

\end{itemize}

\subsection{Search Strategy}
TODO: will fix this (SSS)

As stated before, the dependencies as outputted by the dependency generation
framework allow us to prune impossible inputs. 

\begin{definition}
Let $\mathcal{C} = \{ \Delta_1, \Delta_2, \ldots, \Delta_n \}$ be the set of all
possible changes $\Delta_i$. A change set $c \subseteq \mathcal{C}$ is called a
\emph{configuration}.
\end{definition}

\begin{definition}
An \emph{inconsistent configuration} is a configuration which does not result in
a testable program.
\end{definition}

Inconsistent configurations arise due to semantic and syntactic dependencies
amongst the various syntactic constructs within a program.
%% TODOD: give an example?

\begin{definition}
A test case $c \subseteq \mathcal{C}$ is \emph{$1$-minimal} if $\forall \Delta_i
\in c, c \setminus \{ \Delta_i \}$ is not a failing test case.
\end{definition}

This notion of $1$-minimality was introduced in \citep{zeller99} and this is
what our Semantic Delta-debugging algorithm, \emph{sddmin}, will be aiming to
satisfy. 

As stated previously, we view the dependencies detected by the Constraint
Generation framework as defining constraints that need be satisfied in order to
generate valid configurations.

The obvious question that we are left with is how one can use these constraints
to effectively search through the space of configurations to find a minimal test
case. We will be explaining our algorithm and the heuristic search it carries
out in order to find the $1$-minimal in terms of statistics associated with
nodes in the \emph{dependency graph}.

\begin{definition}
A \emph{dependency graph} associated with a set of syntactic constructs
$\mathcal{C}$ is a directed graph $G$ where the vertex set is given by
$\mathcal{C}$ and a directed edge exists between nodes $\Delta_i$ and $\Delta_j$
iff \emph{dependsOn($\Delta_i$, $\Delta_j$)} holds.
\end{definition}

\begin{definition}
A syntactic construct $\Delta_i$ is a \emph{dependent} of another syntactic
construct $\Delta_j$ if there exists a directed path from $\Delta_i$ to
$\Delta_j$ of length $0$ or greater in the dependency graph. Thus a node is
always dependent on itself.
\end{definition}

\begin{definition}
A syntactic construct $\Delta_i$ is an \emph{enabler} of another syntactic
construct $\Delta_j$ iff $\Delta_j$ is a dependent of $\Delta_i$.
\end{definition}

The algorithm, \emph{sddmin}, then proceeds as follows. We begin with a
dependency graph, $G$, where none of the vertices labeled. We then pick a node,
$\Delta_i$, according to some search heuristic and compute its dependent set,
\emph{Dependent($\Delta_i$)}. We remove all of the nodes in
\emph{Dependent($\Delta_i$)} from the current configuration, $\mathcal{C}$, and
end up with the new configuration $\mathcal{C}^{'}$. 

If $\mathcal{C}^{'}$ still fails in the same way as $\mathcal{C}$, then
$\mathcal{C}^{'}$ is a smaller test case and we have managed to reduce the size
of the failing test. In such a case, repeatedly picking another node from the
newly reduced test case $\mathcal{C}^{'}$ will result in progressively smaller
test cases.

If the the test case does not fail in the same way, then we know that either
$\mathcal{C}^{'}$ passes the test or that results in an inconsistent
configuration. If $\mathcal{C}^{'}$ passes the test we mark $\Delta_i$ and all
of its enablers as ``ESSENTIAL''. If $\mathcal{C}^{'}$ results in an
inconsistent configuration, we label the node $\Delta_i$ as ``INCONSISTENT''.
In both cases the next step is achieved by picking another node from
$\mathcal{C}$ (as per the search heuristic).

This process is repeated till we have no unmarked nodes in our current
configuration. If our constraint generation framework resulted in perfect
dependencies, i.e., it is both sound and complete, then we can stop. The
completeness of the constraints would guarantee that we would never have had to
mark a node as ``INCONSISTENT'' and the soundness guarantees that our resulting
configuration is truly minimal.
%% TODO: elaborate on true minimality (SSS)

If, however, we guarantee only that our constraint system is sound, i.e., we
have no spurious dependencies (though we may be missing some), then that means
that all the nodes marked ``ESSENTIAL'' are truly essential, but the nodes
marked ``INCONSISTENT'' may not form a minimal set. To remedy this, in the
second phase of the algorithm we proceed by removing every ``INCONSISTENT'' node
in the current configuration one at a time till we have guaranteed
$1$-minimality.
%% TODO: this is probably cryptic. elaborate more (SSS)
%% i don't know that this "guarantees" anything, what if there's somehow a
%% recursive dependency between two source lines that are both inconsistent? (ALC)

Alternatively, if we guarantee that our constraint system is complete, i.e., we
may have spurious dependencies, but we never miss a dependency, then that means
that there won't be any node marked ``INCONSISTENT'', but the nodes marked
``ESSENTIAL'' may not truly be essential for causing failure. It is also
possible that we may be guaranteed neither completeness nor soundness of our
constraint system. In either of these cases the way to ensure $1$-minimality of
our final configuration is to proceed (in the second phase of the algorithm) by
removing every node from the current configuration one at a time till we have
guaranteed $1$-minimality.
%% TODO: ugh all this should probably be rewritten. (SSS)

Once the second phase of the algorithm terminates (and it is guaranteed to do
so) our resulting test configuration is $1$-minimal. The details of our Semantic
Delta-debugging algorithm are summarized in \autoref{tbl:sddmin}.


\begin{table*}
\centering
\begin{tabular}{| p{0.97\textwidth} |} \hline
Fill in algorithm description.
%% \begin{algorithmic}

%% \end{algorithmic}

\\  \hline
\end{tabular}
\caption{Some Typical Commands}
\label{tbl:sddmin}
\end{table*}


The only thing that is left for us to define is the search heuristic
employed. The search strategy can be approached using either common approach:
\begin{itemize}
\item \emph{Top-down}: Give more preference to nodes with larger dependent
  sets, i.e., start by removing top-level declarations (and things that
  require them).
\item \emph{Bottom-up}: Give a higher preference to nodes with larger enabler
  sets, i.e., work your way backwards from statements which make use of
  constructs (variables, functions etc.) to statements where these constructs
  are declared.
\item \emph{Mixed}: Use one of the search heuristics as the primary heuristic
  and use the other to break ties.
\end{itemize}
%% It is not yet clear which method is superior, but the proposed dependency
%% modifications should make the bottom-up search strategy a more attractive option
%% than it is in a more traditional, dependency-unaware Delta Debugging.

%% Delta Debugging typically starts off with a standard binary search. As this is
%% possible under a structured framework as well, we may consider introducing
%% heuristics and approximation techniques from graph cutting and image
%% segmentation literature \citep{nc} in order to reduce the occurence of
%% drastic changes (eg., removing a base class on which all classes may depend).


\section{Evaluation}
TODO: will fix this (SSS)

Our evaluation will proceed in two stages.  First, we will compare the number of
iterations required to find a minimal input by our algorithm and the ddmin
algorithm of \citeauthor{dd}.  The final output of our algorithm may be slightly
larger as we will not be shortening variable names or explicitly discarding
whitespace.  In order to ensure that the results we get are comparable we will
run ddmin at the same syntactic granularity as our algorithm. We will perform
these tests on C-based test cases where ddmin is known to reduce the input size
appreciably.

Next, we will examine larger test cases with complex internal dependencies for
which ddmin is unsuitable.  We have generated larger C files with complex
dependencies between structures using composition instead of inheritance.  This
will capture the spirit of the underlying problem without requiring the full
machinery to handle all of C++.  Our goal with these inputs will be to show that
our algorithm can reduce the input size appreciably in a reasonable number of
iterations in cases where ddmin fails to converge after several days. 

%% We will also attempt to compare these results against the hierarchical
%% delta-debugging algorithm of \citeauthor{hdd} (subject to being able to run
%% their code).

For both stages of our evaluation our test cases would consist of source code
that we will be trying to compile.


\section{Related Work}

%% We can either leave this as is or move the discussion of hierarchical
%% delta-debugging earlier. it is up to you (SSS).

%% SSS
%%   I vote we leave this here and 'greatly' extend it, which i'm willing to do.
%% ALC

There has been some prior work in the area with, perhaps, the one most closely
related to ours being the Hierarchical Delta Debugging algorithm as proposed by
\citet{hdd} which works on tree-structured input. However, while
the algorithm can operate on the AST of a program it is unable to account for
dependencies arising from non-ancestral nodes due to declarations. Specifically,
it accounts for \emph{syntactic} dependencies, but not \emph{semantic} ones.

There have been other extensions to the delta debugging algorithm as well, with
variants such as Artho's Iterative Delta Debugging \citep{idd} directing a search
along the axis of version control commits. \citet{smt} used a
variant of the Hierarchical Delta-Debugging algorithm, using the knowledge of
formula structures and types to speed up the delta-debugging process on SMT
formulas.

Our constraint generation step can also be seen as a special case of program
slicing \citep{weiser81} \citep{tip94}. These techniques ease debugging by
removing irrelevant portions of a failing program and can be done either
statically or dynamically (with respect to a concrete run)
\citep{agrawal90}. There is also ongoing work in the area of \emph{executable}
program slices where the goal is to slice a program and output source code
that can still be compiled and run \citep{horwitz10}. One strength of our
technique is that in combining program slicing with a technique as failure
resistant as delta-debugging we do not need to be as precise in our
analysis. Additionally, in cases where we are testing a compiler we can be much
more aggressive than is possible with slicing.


\section{Conclusions and Future Work}
TODO: will add conclusions (SSS)

The most immediately accessible extension to the project would be supplementing
the Clang front-end with support for the remaining unsupported C constructs (or
Due to time constraints, the visitors for certain infrequently used constructs
were not created for use during the Constraint Generation process. As the Clang
ASTVisitor class (used by this implementation) supports visiting all C and all
C++ nodes, extending support to the full spectrum of C/C++-language constructs
is as simple as adding the missing visitors according to the established
patterns of similar features in this implementation. In support of our language-
agnostic approach, we made a concerted effort to ensure that the solution would
be readily extensible to other languages (e.g. Python, OCaml, etc.) with the
translatable ideas and reusable components developed over the course of this
project.

While the ideas are translatable, there would be certain language-specific
challenges to overcome depending on the language and its associated AST module.
In this project, for example, we found that Clang's AST objects often had
incorrect 'end-of-range' source locations associated with them, a drawback which
goes largely unnoticed in most situations as the 'start-of-range' source
location is

%% SSS
%%   I'm thinking we should say something about the carat under the first letter
%%   being the only commonly used extension for the start-end source ranges.
%%   What do you think?
%% ALC

Python, on the other hand, would present a distinct new set of challenges and
opportunities. As a dynamically typed language, Python offers certain challenges
with regards to the interprocedural dataflow analysis necessary to generate the
dependencies. It would still be feasible, however, since the subsequent analysis
requires neither sound nor complete generation of dependencies. At the same
time, using Python's AST module as a front-end (analogous to the Clang AST
front-end employed by our solution) would make certain aspects easier as one
could work at the AST level and convert the results to executable code directly
without the need to manipulate source ranges, or even program text.

\section{Acknowledgments}

Special thanks to Tristan, who helped us navigate our way around Clang's API.

\bibliographystyle{abbrvnat}
\bibliography{refs}

\end{document}
